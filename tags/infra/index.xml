<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>infra on shank</title>
    <link>https://shankisme.com/tags/infra/</link>
    <description>Recent content in infra on shank</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Sat, 02 Mar 2024 15:35:21 +0800</lastBuildDate><atom:link href="https://shankisme.com/tags/infra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>infra intro</title>
      <link>https://shankisme.com/posts/infra/</link>
      <pubDate>Sat, 02 Mar 2024 15:35:21 +0800</pubDate>
      
      <guid>https://shankisme.com/posts/infra/</guid>
      <description>业务关键词 环卫领域 自动驾驶清扫车 城市街道/园区/隧道 摄像头/激光雷达/车辆运行数据 路线规划 远程操控 手自动模式切换 需求关键词 业务/算法部门项目独立部署 项目版本管理 版本迭代方便 资源限额（cpu、内存、GPU） 边缘集群网络高负载(流量大于1Gbps) 技术关键词 边缘计算 弱网环境 边缘部署轻量k8s集群(k3s) 边缘混合架构(ARM + X86) 多集群管理 项目管理 基于k8s的多集群管理平台cloud cloud架构 边缘集群架构 Network Server架构 集群注册流程图 边缘容器网络方案迭代 v1 pod网络分配：k8s.controller-manager.ipam-controller 本机pod ip管理：host-local 同主机pod通信：bridge 跨主机pod通信：k3s内置flannel(vxlan) v2 pod网络分配：k8s.controller-manager.ipam-controller 本机pod ip管理：host-local 同主机pod通信：bridge 跨主机pod通信：flannel pod(vxlan) v3 pod网络分配：k8s.controller-manager.ipam-controller 本机pod ip分配：host-local 同主机pod通信：bridge 跨主机pod通信：自研静态路由配置工具router v4 pod网络分配：在kubelet注册node前，在node中预置pod cidr 本机pod ip分配：host-local 同主机pod通信：bridge 跨主机pod通信：自研静态路由配置工具router </description>
    </item>
    
    <item>
      <title>kubernetes故障排查记录</title>
      <link>https://shankisme.com/posts/kubernetes%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5/</link>
      <pubDate>Fri, 01 Mar 2024 01:35:21 +0800</pubDate>
      
      <guid>https://shankisme.com/posts/kubernetes%E6%95%85%E9%9A%9C%E6%8E%92%E6%9F%A5/</guid>
      <description>本文整理了在工作过程中经历的部分kubernetes故障排查记录。
1. k8s网络直接路由模式下网卡流量异常 起因：node 1到node 2的通信延迟比其他集群高，经过vnstat监听node 2 网卡发现eth0几乎没流量，eth1流量高达几百Mbps，几乎打满带宽。
确认问题：k8s节点node 1 eth0到node 2的流量预期经过node 2 eth0，但是却在node 2的eth1网卡监听到，流量链路异常。
分析：在和运维同学沟通确认了接线和路由配置没问题后，画出简易网络拓扑：
确认三层网络正常，问题发生在二层数据链路层，而二层使用MAC地址定位出口网卡，查看node 1记录的目标ip 192.168.4.112对应的MAC地址发现是192.168.3.112的MAC地址，说明node 1 arp洪泛学习了错误的MAC地址。
解决方案： ip neighbour删除arp记录让node 1重新学习。学习错误MAC地址的原因待观察。
2. internal ip在集群安装和投产时不一致导致API Server向kubelet的请求返回x509 ip非法错误。 组建k3s/k8s集群时，kubelet初始化会经历两个步骤：
选择默认路由的网卡ip作为节点internal ip 主节点将kubelet设置的当前节点internal ip，加入kubelet服务端证书的ip白名单, 签发并下发证书，供节点的kubelet作为服务端tls证书使用 背景: 我们的k3s/k8s集群主要用于边缘计算，使用物联网卡接入公网，流量费用较贵，因此为了在投产前能更经济更快地把基础服务的镜像拉下来，我们上游运维人员会在初始化集群时给gateway节点/主节点接入网线，加入办公室网络，其他从节点将主节点作为默认网关拉镜像。因此在初始化集群时，gateway节点会多出来个网卡，作为访问默认网关的网卡使用。
问题：集群的基础服务的容器全部running后，运维人员给集群断开办公室网络，交付使用。发现在gateway节点使用kubectl工具无法访问pod的日志，返回报错:
Error from server: Get &amp;#34;https://192.168.3.123:10250/containerLogs/&amp;lt;NAMESPACE&amp;gt;/&amp;lt;POD_NAME&amp;gt;/&amp;lt;CONTAINER_NAME&amp;gt;&amp;#34;: x509 certificate is valid for 127.0.0.1, 172.29.1.100, not 192.168.3.123 分析： 日志请求的链路上遇到了TLS双向验证失败的问题，具体原因是某个服务在TLS握手阶段拒绝了请求方，因为请求方的ip未在该服务ip白名单内，而白名单是写进x509证书中的，进一步根据10250端口定位该服务是kubelet，也就是说，使用serving-kubelet.crt作为服务端证书的kubelet没有放行API Server的访问容器日志的请求。
询问运维同学，了解了初始化集群到投入使用的过程：
定位到问题后，找到机器上serving-kubelet.crt证书的位置，openssl x509命令查看证书内容：
$ sudo openssl x509 -in serving-kube-apiserver.crt -text Certificate: ... X509v3 Subject Alternative Name: DNS:kubernetes, DNS:kubernetes.</description>
    </item>
    
    <item>
      <title>ArgoCD核心组件源码浅析</title>
      <link>https://shankisme.com/posts/argocd%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E6%BA%90%E7%A0%81%E6%B5%85%E6%9E%90/</link>
      <pubDate>Thu, 29 Feb 2024 18:32:21 +0800</pubDate>
      
      <guid>https://shankisme.com/posts/argocd%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E6%BA%90%E7%A0%81%E6%B5%85%E6%9E%90/</guid>
      <description>在上一章ArgoCD的设计与功能中，我们介绍了与ArgoCD相关的抽象概念：GitOps、IaC、DevOps，并梳理了ArgoCD的核心组件：Application Controller、Repository Server、API Server
由于API Server的功能在于提供RPC/REST接口和UI界面展示集群资源状态，与GitOps的实现关系不大（正如Core部署模式去掉了API Server），本文将从源码角度分析ArgoCD组件Application Controller、Repository Server的功能实现。
Application Controller Application(以下简称App)是ArgoCD在K8S中注册的自定义资源(CRD)，App Controller是ArgoCD实现的K8S控制器，它连续监视App CR, 并将CR中描述的期望状态和集群实际状态对比，若不一致，将采取纠正措施将二者同步。ArgoCD将集群连接信息和部署的项目信息以及资源清单渲染参数存入了Application CR中，一个最小化的App如下：
apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: guestbook namespace: argocd spec: project: default source: repoURL: https://github.com/argoproj/argocd-example-apps.git targetRevision: HEAD path: guestbook destination: server: https://kubernetes.default.svc namespace: guestbook 其中source表示git仓库和部署项目路径分析信息，destination表示部署的目标集群，在argocd中，可以有多个项目和集群被管理，他们的信息以k8s资源secret的形式存储在etcd中：
// cluster info apiVersion: v1 data: config: ... // 身份信息、用于TLS双向验证的对端CA证书 name: cluster-1 // 集群名 server: ... // 目标集群API Server的URL kind: Secret metadata: annotations: managed-by: argocd.argoproj.io creationTimestamp: &amp;#34;2022-08-05T03:14:07Z&amp;#34; labels: argocd.argoproj.io/secret-type: cluster name: cluster-1 namespace: argocd resourceVersion: &amp;#34;443946&amp;#34; selfLink: /api/v1/namespaces/argocd/secrets/cluster-1 uid: b6f3ff89-782d-426e-aeaf-0c431f7f84bb type: Opaque // project info apiVersion: v1 data: name: Zmxhbm5lbA== sshPrivateKey: .</description>
    </item>
    
    <item>
      <title>ArgoCD的设计和功能</title>
      <link>https://shankisme.com/posts/argocd%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%92%8C%E5%8A%9F%E8%83%BD/</link>
      <pubDate>Wed, 28 Feb 2024 21:32:21 +0800</pubDate>
      
      <guid>https://shankisme.com/posts/argocd%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%92%8C%E5%8A%9F%E8%83%BD/</guid>
      <description>GitOps 基础设施即代码 在理解GitOps前，我们需要先理解什么是基础设施即代码(Infrastructure as Code, IaC)。基础设施即代码表示使用代码来定义基础设施，研发人员可以像对待应用软件一样对待基础设施，例如：
可以创建包含基础架构规范的声明式配置文件，从而便于编辑和分发配置 可以确保每次配置的环境都完全相同 可以进行版本控制 可以将基础设施划分未若干个模块化组件。，并通过自动化以不同的方式进行组合 什么是GitOps GitOps = IaC + Git + CI/CD，即基于IaC的版本化CD/CD。它的核心是使用Git仓库管理基础设施和应用的配置，并且以Git仓库作为基础设施和应用的单一事实来源。Git仓库中的声明式配置描述了目标环境当前所需基础设施地期望状态，借助于GitOps，如果集群地市级状态与Git仓库中的定义与期望状态不匹配，Kubernetes reconcilers会根据期望撞他ilai调整当前地状态，最终使实际状态符合期望状态。
GitOps vs DevOps 从广义上来看，GitOps与DevOps并不冲突，GitOps是一种技术手段，而DevOps是一种文化。GitOps是一种实现持续交付、持续部署和基础设施即代码地工具和框架，它是支持DevOps文化的。 GitOps和DevOps主要区别
GitOps以目标为导向，使用Git维护期望状态，并不断调整实际状态，最终与期望状态相匹配。而DevOps更多关注的是最佳实践，这些实践可以应用于企业每一个流程。 GitOps采取声明式的操作方法，而DevOps同时接受声明式和命令式的方法。 ArgoCD的优势 Git作为应用的唯一真实来源：期望状态的唯一存储 快速回滚：通过Git History切换commit将应用状态快速恢复到上一个可用的状态 集群灾备：若集群A出现故障，可以直接创建新集群，然后将ArgoCD连接到包含集群A所有配置声明的Git仓库，最终新集群B的状态会与旧集群状态一致，无需人工干预 ArgoCD架构 从功能架构来看，Argo CD 主要有三个组件：API Server、Repository Server 和 Application Controller。从 GitOps 工作流的角度来看，总共分为 3 个阶段：检索、调谐和呈现
检索 - Repo Server 检索阶段会克隆应用声明式配置清单所在的 Git 仓库，并将其缓存到本地存储。包含 Kubernetes 原生的配置清单、Helm Chart 以及 Kustomize 配置清单。履行这些职责的组件就是 Repository Server。
调谐 - Application Controller 调谐（Reconcile）阶段是最复杂的，这个阶段会将 Repository Server 获得的配置清单与反映集群当前状态的实时配置清单进行对比，一旦检测到应用处于 OutOfSync 状态，Application Controller 就会采取修正措施，使集群的实际状态与期望状态保持一致。</description>
    </item>
    
  </channel>
</rss>
